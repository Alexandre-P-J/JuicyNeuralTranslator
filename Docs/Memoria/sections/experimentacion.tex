\chapter{Implementación de modelos de traducción}

\section{Métricas de valoración}
En el trabajo se usan las métricas BLEU y chrF para la comparación de textos obtenidos con modelos de traducción respecto a sus textos de referencia. Ambas métricas pretenden dar una estimación de la calidad de las traducciones sobre un corpus.

Las métricas elegidas son simples, fáciles de computar y bien conocidas, además son independientes del tokenizador usado.
BLEU es una de las métricas más famosas, sin embargo, a lo largo de los años ha sido objeto de crítica debido a su baja correlación con la valoración humana y otros problemas. ChrF es una métrica más moderna que usa n-gram de caracteres en vez de palabras. Esto permite la puntuación parcial de palabras con erratas y es especialmente relevante en lenguas con morfología rica como el catalán o el español. Se ha demostrado empíricamente que chrF tiene una mejor correlación con la valoración humana que BLEU \cite{Ma2019Aug} y es una métrica generalmente recomendada \cite{Kocmi2021Jul} y simple de calcular debido a que no requiere preentrenamiento.

\subsection{\textit{SacreBleu}}
\textit{SacreBleu} \cite{Post2018Oct} es una implementación de la puntuación BLEU con el objetivo de estandarizar el cálculo de la métrica y facilitar la comparación.
BLEU compara una secuencia de texto con una o más secuencias de referencia mediante una puntuación entre el 0 y el 100. Sin embargo en este trabajo cada secuencia únicamente corresponde a una secuencia de referencia.

\begin{figure}[H]
    \begin{align*}
        BLEU(N,S_{p},S_{r}) = \text{Precisión}_{media}(N, S_{p}, S_{r})\cdot \text{Penalización}_{brevedad}(S_{p}, S_{r})
    \end{align*}
    \caption{Fórmula de la métrica BLEU [Elaboración propia]}\label{Bleuform}
\end{figure}
La figura \ref{Bleuform} muestra una formula para calcular BLEU a partir de $N \geq 1$, un natural que normalmente es $4$ y $S_{p}$, $S_{r}$ la frase predicha por un modelo y su secuencia de referencia en ese orden.

\begin{figure}[H]
    \begin{align*}
        \text{Sea }C_{p}\text{ el conjunto de }n\text{-grams de }S_{p}\text{,}\\
        T(S_{x}, g)\text{ el número de ocurrencias de }g\text{ en }S_{x}\text{ y}\\
        |S_{p}|\text{ el número de }n\text{-grams de la secuencia }S_{p}\text{,}\\ \\
        \text{Precisión}(n, S_{p}, S_{r}) = \sum_{g\in{C_{p}}}\frac{min(T(S_{p}, g), T(S_{r}, g))}{ |S_{p}| }
    \end{align*}
    \caption{Precisión de los $n$-gram de una secuencia predicha sobre una secuencia de referencia [Elaboración propia]}\label{Bleuprecision}
\end{figure}

La fórmula \ref{Bleuprecision} calcula la precisión de $S_{p}$ sobre $S_{r}$ dividiendo el número de n-gram correctamente predichos entre el total de n-gram predichos. Sin embargo, para evitar que los n-gram repetidos más veces de las que aparecen aumenten la precisión, se limita el conteo al mínimo número de apariciones en ambas secuencias.

\begin{figure}[H]
    \begin{align*}
        \text{Precisión}_{media}(N, S_{p}, S_{r}) = \prod^{N}_{n=1}{\text{Precisión}(n, S_{p}, S_{r})^{\frac{1}{\text{N}}}}
    \end{align*}
    \caption{Media geométrica de las $N$ precisiones calculadas [Elaboración propia]}\label{Bleumean}
\end{figure}

En la figura \ref{Bleumean} se observa la expresión del primer término de la primera fórmula \ref{Bleuform}. Se calcula la media geométrica de las precisiones obtenidas usando 1-gram..N-gram en la fórmula \ref{Bleuprecision}.

Por último, en la figura \ref{Bleupenal} se describe el segundo término de la fórmula para calcular la métrica BLEU. Se trata de un factor que penaliza las secuencias predichas más cortas que su correspondiente secuencia de referencia. Esta medida se toma para evitar secuencias de pocos n-gram con alta precisión. Por ejemplo: una secuencia de una sola palabra tendría máxima precisión si la palabra aparece al menos una vez en la secuencia de referencia.

\begin{figure}[H]
    \begin{align*}
        \text{Sea }|S_{x}|\text{ el número de palabras de }S_{x}\text{,}\\ \\
        \text{Penalización}_{brevedad}(S_{p}, S_{r}) =
        \begin{cases}
            1, &\text{si }|S_{p}| > |S_{r}|\\
            e^{(1 - |S_{r}|)/|S_{p}| }, &\text{si }|S_{p}|\leq{ |S_{r}| }
          \end{cases}
    \end{align*}
    \caption{Penalización de la brevedad en la métrica BLEU [Elaboración propia]}\label{Bleupenal}
\end{figure}


\subsection{\textit{Character N-gram F-score}}
ChrF \cite{popovic-2015-chrf} es una métrica más reciente que BLEU para la comparativa automática de traducciones. Con el fin de estandarizar el procedimiento, la implementación usada en el trabajo también la proporciona \textit{SacreBleu} \cite{Post2018Oct}.

\begin{figure}[H]
    \begin{align*}
        \text{Donde }S_p\text{ y }S_r\text{ son la secuencia predicha y de referencia,}\\
        \beta\text{ asigna }\beta\text{ veces más importancia al \textit{recall} que a la precision,}\\
        \text{En el proyecto y en \textit{SacreBleu}, }N=6\text{ y }\beta=2\text{,}\\ \\
        \text{chrF}(\beta, N,S_{p},S_{r})=(1-\beta^{2})\frac{\text{chrP}(N,S_{p},S_{r})\cdot\text{chrR}(N,S_{p},S_{r})}{\beta^{2}\cdot\text{chrP}(N,S_{p},S_{r})+\text{chrR}(N,S_{p},S_{r})}
    \end{align*}
    \caption{Fórmula general de la métrica $\text{chrF}_{\beta}$ [Elaboración propia]}\label{chrfformula}
\end{figure}

La fórmula de la figura \ref{chrfformula} es la misma que se presenta en \cite{popovic-2015-chrf} con todos sus parámetros expuestos para facilitar su entendimiento. Los términos chrP y chrF se definen a continuación.

\begin{figure}[H]
    \begin{align*}
        \text{Sea }C_{r}\text{ el conjunto de }n\text{-grams de }S_{r}\text{,}\\
        T(S_{p}, g)\text{ el número de ocurrencias de }g\text{ en }S_{p}\text{ y}\\
        |S_{p}|\text{ el número de }n\text{-grams de la secuencia }S_{p}\text{,}\\ \\
        \text{Precisión}(n,S_{p},S_{r})=\sum_{g\in{C_{r}}}{\frac{ T(S_{p},g) }{ |S_{p}| }}\\
        \text{chrP}(N,S_{p},S_{r})=\sum_{n=1}^{N}\frac{\text{Precisión}(n,S_{p},S_{r})}{N}
    \end{align*}
    \caption{Media aritmética de la precisión de 1-gram hasta N-gram [Elaboración propia]}\label{chrfprecisionformula}
\end{figure}

La figura \ref{chrfprecisionformula} muestra la fórmula de la precisión en términos de n-grams en secuencias y posteriormente define chrP como la media aritmética de las precisiones obtenidas con n-gram de 1 hasta $N$ elementos.

\begin{figure}[H]
    \begin{align*}
        \text{Sea }C_{r}\text{ el conjunto de }n\text{-grams de }S_{r}\text{,}\\
        T(S_{p}, g)\text{ el número de ocurrencias de }g\text{ en }S_{p}\text{ y}\\
        |S_{r}|\text{ el número de }n\text{-grams de la secuencia }S_{r}\text{,}\\ \\
        \textit{Recall}(n,S_{p},S_{r})=\sum_{g\in{C_{r}}}{\frac{ T(S_{p},g) }{ |S_{r}| }}\\
        \text{chrR}(N,S_{p},S_{r})=\sum_{n=1}^{N}\frac{\textit{Recall}(n,S_{p},S_{r})}{N}
    \end{align*}
    \caption{Media aritmética del \textit{recall} de 1-gram hasta N-gram [Elaboración propia]}\label{chrfrecallformula}
\end{figure}

La figura \ref{chrfrecallformula} expresa la fórmula del \textit{recall} en términos de n-grams en secuencias y define chrR como el promedio de los \textit{recall} obtenidos con n-gram de 1 hasta $N$ elementos.

\section{Metodología}\label{datapipeline}
En los experimentos de \textit{finetuning} y \textit{transfer learning} el \textit{pipeline} de datos es idéntico. Los datasets son corpus biling{\"u}es obtenidos del repositorio OPUS \cite{CORPUS} y están compuestos de pares de secuencias con el mismo contenido en dos idiomas. Los corpus se mezclan aleatoriamente y se parten en conjuntos de entrenamiento, validación y test en las proporciones de 60/20/20\% en ese mismo orden.

Posteriormente, se arreglan en cada conjunto los caracteres unicode inválidos, se elimina la indentación y otros espacios en blanco innecesarios y se descartan las secuencias vacías. Sin embargo, no se eliminan acentos diacríticos u otros elementos que enriquecen el idioma. Luego se tokeniza usando \textit{SentencePiece} tal como se detalla en profundidad en el apartado \ref{datamangle} y se truncan las tokenizaciones a un tamaño máximo de 512 tokens, el límite de los modelos. Posteriormente se agrupan los datos en \textit{batches} de 16 ejemplos listos para ser procesados por el modelo descrito en el capítulo \ref{transformerchapter}, que es idéntico a la arquitecta transformer original.

Se usa el conjunto de entrenamiento para entrenar los modelos y el conjunto de validación permite monitorizar el progreso del aprendizaje manteniendo una buena generalización. Finalmente, para estimar la bondad de los modelos se usa el conjunto de test, nunca antes visto por el modelo, para evaluar su rendimiento.

Para la medición y evaluación se usan las métricas BLEU y chrF sobre el conjunto de validación o test para comparar todas las secuencias traducidas con sus respectivas secuencias de referencia.
Además, aprovechando que los modelos usados forman parte de la iniciativa \textit{Tatoeba Challenge} \cite{tiedemann-2020-tatoeba} entrenados con datasets de OPUS \cite{CORPUS}, también se evaluarán los resultados con los conjuntos de test de Tatoeba.
Antes de computar las métricas chrF y BLEU, se revierte la tokenización y se postprocesan las secuencias eliminando espacios al inicio y final de las frases.


\section{\textit{Finetuning}}\label{finetune}
El primer experimento que se llevó a cabo fue el \textit{finetuning} de un modelo de traducción inglés-español para ajustar su registro a un tono más formal.
El modelo de partida usado es \textbf{Helsinki-NLP/opus-mt-en-es} del repositorio de Huggingface.
Los modelos del grupo de investigación del procesamiento del lenguaje natural de la universidad de Helsinki publicados en Huggingface usan el \textit{framework} MarianMT \cite{Junczys-Dowmunt2018Apr}, con una arquitectura idéntica al transformer original descrito en el capítulo \ref{transformerchapter} en profundidad.

Para el \textit{finetuning} del modelo, se optó por conservar el máximo número de hiperparámetros intactos y simular un \textit{pipeline} lo más parecido posible al original.
La normalización usada en el corpus del modelo original consistía en la eliminación de acentos diacríticos y la limpieza o sustitución de caracteres unicode, sin embargo, para el \textit{finetuning} del modelo se ha preferido no eliminar acentos ni otras riquezas ling{\"u}ísticas con el objetivo de mejorar la calidad del modelo. El resto del \textit{pipeline} de datos coincide lo descrito en el apartado \ref{datapipeline}.
El tokenizador del modelo original usa \textit{SentencePiece} con un vocabulario de 65.000 tokens y se conservó para el nuevo modelo debido a que producía tokenizaciones razonables y de un tamaño similar independiente de la modificación en la normalización.

En la rutina de optimización Adam \cite{Kingma2014Dec} únicamente se redujo el ratio de aprendizaje debido a que manteniendo el valor original, el modelo era muy inestable y empeoraba de forma visible. Se optó por un ajuste manual del hiperparámetro, de $2\cdot 10^{-3}$ a $2\cdot 10^{-5}$, dos órdenes de magnitud inferior.

\subsection{Entrenamiento}
Los corpus de entrenamiento elegidos para ajustar el modelo a un registro formal debían pertenecer a un dominio lo más cercano posible al entorno de aplicación. Se usaron dos datasets de textos legislativos y científicos ya que ambos contextos son formales y podrían beneficiarse una traducción automática más específica.
\subsubsection{JRC-Acquis}
El primer modelo se entrenó con el dataset JRC-Acquis del repositorio OPUS \cite{CORPUS} y es una colección de textos legislativos de la unión europea desde el año 1950 hasta la actualidad. Se ha elegido el par de lenguajes inglés-español alineados por parágrafos con un total de 504.981 pares de secuencias.

Se finalizó el entrenamiento una vez terminado el tercer \textit{epoch} a las 5 horas y media de ejecución debido a que el error de validación empezaba a estabilizarse. No se esperó hasta la convergencia total debido a que el coste temporal y computacional se hubiera disparado y las ganancias probablemente serían mínimas.

\subsubsection{SciELO}
También se hizo \textit{finetuning} con el dataset SciELO del repositorio OPUS \cite{CORPUS}. Este dataset a diferencia de JRC-Acquis, procede de una librería digital de publicación científica de la cual se esperan textos formales y con vocabulario rico y específico de distintos campos. En este caso los textos también se han alineado por parágrafos y hay un total de 416.322 pares de secuencias inglés-español.

De la misma manera que con el dataset anterior, se finalizó el entrenamiento una vez terminado el tercer \textit{epoch} a las 5 horas de ejecución debido a que el error de validación empezaba a estabilizarse. Tampoco se esperó hasta la convergencia total debido al coste temporal y computacional, que se hubiera disparado y las ganancias probablemente serían mínimas.


\subsection{Resultados}
El modelo inicial para efectuar \textit{finetuning} se evalúa en la tabla \ref{originalfinetune} con los conjuntos de validación de ambos datasets usados y con el dataset de Tatoeba propuesto por los creadores del modelo.
\begin{table}[H]
    \begin{center}
        \begin{tabular}{ l S[table-format=2.4] S[table-format=2.4] }
        \multicolumn{3}{l}{\textbf{Helsinki-NLP/opus-mt-en-es}}\\
        \textbf{Muestra} & \textbf{BLEU} & \textbf{chrF} \\
        Validación JRC-Acquis & 55,2525 & 73,6 \\
        Validación SciELO & 40,2099 & 67,248 \\
        Tatoeba \textit{Challenge} & 56,9578 & 73,6682
        \end{tabular}
        \caption{Métricas del modelo original antes del \textit{finetuning} [Elaboración propia]}\label{originalfinetune}
    \end{center}
\end{table}

Se observa que el modelo inicial es razonablemente bueno en la traducción inglés-español y además tiene un buen rendimiento tanto el dataset JRC-Acquis como en SciELO elegidos para el entrenamiento.
En la tabla \ref{finetuneacquis} se muestran las métricas del \textit{finetuning} con el dataset JRC-Acquis.

\begin{table}[H]
    \begin{center}
        \begin{tabular}{ l S[table-format=2.4] S[table-format=2.4] }
        \multicolumn{3}{l}{\textbf{JRC-Acquis \textit{finetuned}}}\\
        \textbf{Muestra} & \textbf{BLEU} & \textbf{chrF} \\
        Validación JRC-Acquis & 58,6264 & 75,6313 \\
        Test JRC-Acquis & 58,621 & 75,654 \\
        Tatoeba \textit{Challenge} & 39,4177 & 61,0986
        \end{tabular}
        \caption{Métricas del modelo \textit{finetuned} sobre el corpus JRC-Acquis [Elaboración propia]}\label{finetuneacquis}
    \end{center}
\end{table}

Como es de esperar, el rendimiento en el corpus JRC-Aquis mejora, sin embargo la puntuación en Tatoeba decrece sustancialmente. El mismo fenómeno ocurre también en el modelo entrenado con SciELO pero a una escala menor según se observa en la tabla \ref{finetunescielo}.

\begin{table}[H]
    \begin{center}
        \begin{tabular}{ l S[table-format=2.4] S[table-format=2.4] }
        \multicolumn{3}{l}{\textbf{SciELO \textit{finetuned}}}\\
        \textbf{Muestra} & \textbf{BLEU} & \textbf{chrF} \\
        Validación SciELO & 43,1173 & 69,0259 \\
        Test SciELO & 43,0942 & 68,9413 \\
        Tatoeba \textit{Challenge} & 50,071 & 69,1425
        \end{tabular}
        \caption{Métricas de modelo \textit{finetuned} sobre el corpus SciELO [Elaboración propia]}\label{finetunescielo}
    \end{center}
\end{table}

La disminución en rendimiento con el test de Tatoeba podría significar que el modelo ha perdido calidad en la traducción inglés-español debido a que los datasets elegidos no fueran representativos al problema a resolver. Sin embargo, probando los modelos con diversos textos de ámbitos y contextos distintos se observa que en la mayoría de casos los modelos entrenados ofrecen traducciones de igual o mayor calidad en algunos casos.

Se cree que el test de Tatoeba pertenece a un dominio con ejemplos más simples del idioma y los datasets usados para el entrenamiento empeoran los resultados en ese test porque a menudo hacen uso de vocabulario y sintaxis más rica incluso cuando no es necesario, sin embargo las métricas usadas no son capaces de percibir la similitud semántica.

Los modelos se han probado por un total de 4 individuos sin contar al implementador y el director del trabajo para realizar un test a ciegas con el objetivo de determinar subjetivamente la calidad de los modelos. Además, uno de los individuos es traductor e intérprete de profesión. Todos han probado los modelos de forma exhaustiva con fragmentos de temática diversa y de libre elección.
El test a ciegas ha dado resultados mejor de lo esperados ya que los 4 participantes votaron el modelo entrenado con el dataset SciELO como el mejor, seguido del modelo de JRC-Acquis y por último el original.

Los participantes mencionaron que el modelo SciELO interpreta mejor el contexto y su vocabulario suele ser más apropiado. El modelo original a veces comete errores en conjugaciones verbales, sin embargo los modelos entrenados conjugan correctamente. Por último, el traductor e intérprete de profesión comentó que el modelo SciELO ofreció una traducción sorprendentemente buena de algunos poemas de Edgar Alan Poe.

\begin{figure}[H]
\subsubsection{Original}
\begin{spverbatim}
Because of its neutrality policy, the Swiss army does not take part in armed conflicts in other countries but is part of some peacekeeping missions around the world. Since 2000 the armed force department has also maintained the Onyx intelligence gathering system to monitor satellite communications.
\end{spverbatim}

\subsubsection{Helsinki-NLP/opus-mt-en-es}
\begin{spverbatim}
Debido a su política de neutralidad, el ejército suizo no participa en conflictos armados en otros países, sino que forma parte de algunas misiones de mantenimiento de la paz en todo el mundo. Desde 2000, el departamento de las fuerzas armadas también ha mantenido el sistema de reunión de información de Onyx para vigilar las comunicaciones por satélite.
\end{spverbatim}

\subsubsection{JRC-Acquis}
\begin{spverbatim}
Debido a su política de neutralidad, el ejército suizo no participa en conflictos armados en otros países, sino que forma parte de algunas misiones de mantenimiento de la paz en todo el mundo. Desde 2000, el departamento de las fuerzas armadas también ha mantenido el sistema de recopilación de información de Onyx para supervisar las comunicaciones por satélite.
\end{spverbatim}

\subsubsection{SciELO}
\begin{spverbatim}
Debido a su política de neutralidad, el ejército suizo no participa en conflictos armados en otros países, sino que forma parte de algunas misiones de mantenimiento de la paz en todo el mundo. Desde el año 2000, el departamento de las fuerzas armadas también ha mantenido el sistema de recolección de inteligencia Onyx para monitorear las comunicaciones por satélite.
\end{spverbatim}
\caption{Ejemplo de traducción [Elaboración propia, fragmento original de \cite{ContributorstoWikimediaprojects2022Jan}]}\label{ejemplotranslate}
\end{figure}

En la figura \ref{ejemplotranslate} se muestra un ejemplo de traducción donde los tres modelos obtienen un resultado aceptable, sin embargo el modelo original confunde la palabra \textit{``gathering''} con ``reunión'', que es incorrecta en el contexto del fragmento, los modelos entrenados usan los verbos ``recopilar'' y ``recolectar'' que sí son correctos.
El modelo entrenado con SciELO interpreta correctamente que ``2000'' hace referencia a un año y genera una traducción más inteligible.
También se destaca el uso del verbo ``monitorear'' por el modelo SciELO, que corresponde perfectamente a \textit{``monitor''}, mientras que el modelo original usa el verbo ``vigilar'' y el modelo JRC-Acquis usa ``supervisar'', siendo el segundo más apropiado. Otro ejemplo similar es el uso del sustantivo ``inteligencia'' en lugar de ``información'' por el modelo SciELO, que conserva mejor la riqueza léxica del fragmento original.


\section{\textit{Transfer learning}}
Debido a la escasez de corpus en catalán combinado a la baja calidad de estos, se considera paliar esta limitación mediante el uso de \textit{transfer learning}.
Este experimento consiste en la implementación de un modelo de traducción catalán-inglés a partir de un modelo español-inglés.
Como modelo de partida para el \textit{transfer learning} se ha usado \textbf{Helsinki-NLP/opus-mt-es-en} del repositorio de Huggingface. También se ha usado el modelo \textbf{Helsinki-NLP/opus-mt-ca-en} como \textit{baseline} ya que se espera poder superar su calidad.

Se ha optado por conservar el máximo número de hiperparámetros intactos así como el \textit{pipeline} de datos. La normalización se ha implementado como en el experimento de \textit{finetuning} descrito en el apartado \ref{finetune} y el resto del \textit{pipeline} es tal como se describe en el apartado \ref{datapipeline}.

Se consideró reentrenar el tokenizador \textit{SentencePiece} con el corpus de entrenamiento catalán-inglés para obtener mejores tokenizaciones, sin embargo se ha decidido conservar el tokenizador original del modelo español-inglés, con un vocabulario de 65.000 tokens, por los motivos siguientes:
\begin{itemize}
    \item El tokenizador \textit{SentencePiece} original mezcla el vocabulario español e inglés en un mismo corpus y de entrenar un nuevo tokenizador, no sería posible conservar los \textit{embeddings} en inglés.
    \item Conservando el tokenizador, el modelo quizás pueda seguir traduciendo del español al inglés después del \textit{transfer learning}.
    \item Empíricamente las tokenizaciones de secuencias en catalán con el tokenizador español-inglés respecto el tokenizador catalán-inglés únicamente contienen entre un 15-20\% más tokens.
\end{itemize}

En el optimizador Adam \cite{Kingma2014Dec} se ha reducido manualmente el hiperparámetro de ratio de aprendizaje, de $2\cdot 10^{-3}$ a $10^{-5}$, dos ordenes de magnitud inferior.


\subsection{Entrenamiento}
Durante entrenamiento del modelo se congeló la matriz de \textit{embeddings} del \textit{decoder} correspondiente a la matriz del idioma inglés ya que es común tanto en el modelo original español-inglés como en el modelo catalán-inglés que se quiere implementar.

Los datos de entrenamiento se eligieron a partir de los corpus usados por el modelo catalán-inglés entrenado por Helsinki-NLP. Ellos seleccionaron los datasets Books, EUbookshop, GlobalVoices, GNOME, KDE4, OpenSubtitles, QED y Ubuntu del repositorio OPUS \cite{CORPUS}. Sin embargo, tras una examinación de los corpus, se percibió que el dataset EUbookshop contenía mucho ruido, por ejemplo: links, siglas, símbolos inválidos y muchas secuencias sin valor semántico. Por otra parte, los corpus GNOME, KDE y Ubuntu extraídos de los software con su mismo nombre, contenían secuencias con vocabulario y semántica muy pobre además de cortas debido al origen y contexto de los datos, las interfaces gráficas de programas.

Para el \textit{transfer learning} solo se usaron los corpus Books, GlobalVoices, OpenSubtitles y QED compuestos por textos alineados en parágrafos de la literatura, noticias internacionales, subtítulos de películas y de medios educativos en ese orden.
El dataset resultante después de mezclar los corpus aleatoriamente como se describe en \ref{datapipeline} contiene un total de 509.194 secuencias catalán-inglés.

Se dio por terminado el entrenamiento una vez concluido el \textit{epoch} 5 después de 9 horas de entrenamiento debido a que el error de validación empezaba a estabilizarse pero no se esperó hasta la convergencia total debido al coste computacional y temporal asociado. 

\subsection{Resultados}
\begin{table}[H]
    \begin{center}
        \begin{tabular}{ l S[table-format=2.4] S[table-format=2.4] }
        \multicolumn{3}{l}{\textbf{Helsinki-NLP/opus-mt-es-en}}\\
        \textbf{Muestra} & \textbf{BLEU} & \textbf{chrF} \\
        Tatoeba \textit{Challenge} Ca-En & 9,6952 & 28,3868 \\
        Tatoeba \textit{Challenge} Es-En & 60,6793 & 74,9145
        \end{tabular}
        \caption{Métricas del modelo original español-inglés [Elaboración propia]}\label{transferorigesen}
    \end{center}
\end{table}

La tabla \ref{transferorigesen} muestra el rendimiento del modelo original sobre los tests de Tatoeba para los pares catalán-inglés y Español-Inglés. Como es de esperar, el modelo obtiene una puntuación muy baja en catalán-inglés ya que no ha sido entrenado para ese par de lenguajes.
La puntuación obtenida en el test Español-Inglés es muy elevada. Si el test es representativo a la tarea de traducción, se podría interpretar que el modelo realiza traducciones de buena calidad.

\begin{table}[H]
    \begin{center}
        \begin{tabular}{ l S[table-format=2.4] S[table-format=2.4] }
        \multicolumn{3}{l}{\textbf{Helsinki-NLP/opus-mt-ca-en}}\\
        \textbf{Muestra} & \textbf{BLEU} & \textbf{chrF} \\
        Tatoeba \textit{Challenge} Ca-En & 48,7704 & 65,7556 \\
        Tatoeba \textit{Challenge} Es-En & 17,6634 & 36,9217
        \end{tabular}
        \caption{Métricas del modelo \textit{baseline} catalán-ingles [Elaboración propia]}\label{transferorigcaen}
    \end{center}
\end{table}

Se han calculado las mismas métricas para el modelo \textit{baseline} catalán-inglés. La tabla \ref{transferorigcaen} muestra un mal rendimiento para la traducción español-inglés y bueno para el par de lenguajes catalán-inglés. Los resultados confirman que el modelo catalán-inglés entrenado por el equipo NLP de Helsinki no se ha implementado mediante \textit{transfer learning} desde un modelo español-inglés como se realiza en este trabajo. De ser así, se esperaría una puntuación mayor en el segundo test, reminiscente del modelo original.
Esta conclusión no es sorprendente ya que el equipo de Helsinki describe el procedimiento de entrenamiento en el repositorio del modelo. 

\begin{table}[H]
    \begin{center}
        \begin{tabular}{ l S[table-format=2.4] S[table-format=2.4] }
        \multicolumn{3}{l}{\textbf{Catalán-Inglés \textit{transfer learning}}}\\
        \textbf{Muestra} & \textbf{BLEU} & \textbf{chrF} \\
        Validación & 35,7988 & 55,3232 \\
        Test & 35,8034 & 55,2997 \\
        Tatoeba \textit{Challenge} Ca-En & 53,54 & 69,2849 \\
        Tatoeba \textit{Challenge} Es-En & 57,049 & 72,4531
        \end{tabular}
        \caption{Métricas del modelo entrenado con \textit{transfer learning} de español-inglés a catalán-inglés [Elaboración propia]}\label{transfercaen}
    \end{center}
\end{table}

La tabla \ref{transfercaen} muestra las métricas obtenidas en el modelo resultado del \textit{transfer learning} a partir del modelo español-inglés. La puntuación obtenida en los conjuntos de validación y test muestran unos valores aceptables pero es difícil interpretar la calidad del modelo entrenado. Las métricas sobre los conjuntos de Tatoeba superan al modelo \textit{baseline} para la traducción catalán-inglés. Además, conserva una puntuación muy alta en la traducción español-inglés procedente del modelo original.

La diferencia entre la puntuación del modelo en su conjunto de test y el de Tatoeba parece indicar que los datos de Tatoeba són más simples. Además, a diferencia de los otros tests procedentes de Tatoeba, el número de ejemplos del test catalán-inglés es un orden de magnitud más pequeño que el resto.

Se ha decidido realizar un experimento a ciegas tal como se hizo con los modelos de \textit{finetuning} \ref{finetune}. La prueba se ha realizado con los mismos 4 individuos y el objetivo era comparar a ciegas el modelo obtenido mediante \textit{transfer learning} con los modelos catalán-inglés y español-inglés del equipo NLP de Helsinki. Los 4 participantes usaron fragmentos de temática diversa y de libre elección y al finalizar el experimento, coincidieron en que el modelo entrenado mediante \textit{transfer learning} ofrece traducciones generalmente mejores que el modelo catalán-inglés \textit{baseline}. También se cree que usa un vocabulario más apropiado y conserva mejor la semántica con menos erratas.

Sin embargo, al comparar el modelo entrenado contra el modelo original español-inglés, destacaron que las traducciones parecen ser idénticas con ligeras diferencias de vocabulario. Dos de los participantes votaron por el modelo original, uno eligió el modelo entrenado y otro decidió no votar a favor ni en contra de ninguno.

Dado que el objetivo era el entrenamiento de un modelo catalán-inglés a partir del modelo original español-inglés y el modelo se ha valorado mejor que el \textit{baseline}, se considera el experimento un éxito. Por otra parte, se ha logrado conservar la habilidad del modelo para la traducción al inglés desde el español.

\begin{figure}[H]
\subsubsection{Original}
\begin{spverbatim}
Sloane Street és un carrer de Londres orientat nord-sud des de Knightsbridge fins a Sloane Square; creua Pont Street més o menys a la meitat i està situada completament en el districte reial de Kensington i Chelsea. Sloane Street deu el seu nom a Sir Hans Sloane, que va adquirir els terrenys veïns el 1712.
\end{spverbatim}

\subsubsection{Helsinki-NLP/opus-mt-ca-en}
\begin{spverbatim}
Sloane Street is a North-sud London street from Knightsbridge to Sloane Square; cross Street more or less than half and is completely located in Kenston's royal district and Chelsea. Selane Street owes Sir Hans Selane's name, who acquired the neighbours' ground in 1712.
\end{spverbatim}

\subsubsection{Modelo obtenido mediante \textit{transfer learning}}
\begin{spverbatim}
Sloane Street is a north-south street in London from Knightsbridge to Sloane Square; it crosses Pont Street more or less halfway and is located entirely in the royal district of Kensington and Chelsea. Sloane Street owes his name to Sir Hans Sloane, who acquired the lands next to him in 1712.
\end{spverbatim}
\caption{Ejemplo de traducción [Elaboración propia, fragmento original de \cite{ContributorstoWikimediaprojects2021Aug}]}\label{transferexample}
\end{figure}

La figura \ref{transferexample} muestra un ejemplo de traducción catalán-inglés. El modelo \textit{baseline} no traduce correctamente la expresión \textit{``més o menys a la meitat''} ya que ``more or less than half'' carece de sentido. La traducción ``Kenston's royal district and Chelsea'' para el fragmento \textit{``districte reial de Kensington i Chelsea''} es incorrecto ya que se trata de un único distrito. Además, la última frase del fragmento es confusa e incorrecta. Por otra parte, la traducción ofrecida por el modelo obtenido mediante \textit{transfer learning} es correcta exceptuando la partícula \textit{``his''} que debería ser \textit{``its''}.